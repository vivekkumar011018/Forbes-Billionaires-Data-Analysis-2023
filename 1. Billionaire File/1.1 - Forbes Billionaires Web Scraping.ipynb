{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83da15e4",
   "metadata": {},
   "source": [
    "##  import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver #importing web driver\n",
    "from selenium.webdriver.chrome.service import Service # importting chrome service\n",
    "from selenium.webdriver.chrome.options import Options # importing options to customize the behavior of the Chrome browser \n",
    "from selenium.webdriver.common.by import By # importing By to locate elements on a web page.\n",
    "from bs4 import BeautifulSoup \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba5f33",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db28176",
   "metadata": {},
   "source": [
    "##### Here I am scrapping all billionaires data using this link \"https://www.forbes.com/real-time-billionaires/#60825e283d78\", but there is a problem here. \n",
    "1. I need a column industry but there is no specific column for industry. So I will have to manually scrape by each industry, so that I can create a new column called **Industry**.\n",
    "2. In this billionaires data, female is also there but not specified in webpage. Here I will also manually scrape females name so that I can also create a column called **Sex** that will specify male and female billionaires.\n",
    "\n",
    "##### Lets's Scrape by keeping this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d52c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service('C:/Users/Vivek Kumar/Desktop/chromedriver.exe') # Importing chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.forbes.com/real-time-billionaires/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eada0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I have taken all the XPATH of industries and stored in a distionary for further use.\n",
    "industry_path = {\n",
    "    \"Automotive\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[2]',\n",
    "    \"Construction & Engineering\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[3]',\n",
    "    \"Diversified\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[4]',\n",
    "    \"Energy\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[5]',\n",
    "    \"Fashion & Retail\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[6]',\n",
    "    \"Finance & Investments\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[7]',\n",
    "    \"Food & Beverage\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[8]',\n",
    "    \"Gambling & Casinos\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[9]',\n",
    "    \"Healthcare\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[10]',\n",
    "    \"Logistics\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[11]',\n",
    "    \"Manufacturing\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[12]',\n",
    "    \"Media & Entertainment\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[13]',\n",
    "    \"Metals & Mining\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[14]',\n",
    "    \"Real Estate\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[15]',\n",
    "    \"Service\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[16]',\n",
    "    \"Sports\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[17]',\n",
    "    \"Technology\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[18]',\n",
    "    \"Telecom\":'/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/div/ul/li[19]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a20153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our webpage, there are two scrollers, 'a scroler inside a scroller'. \n",
    "#So, Firstly I will locate outer scroller, then I will locate inner scroller, inside the outer scroller.\n",
    "time.sleep(5)\n",
    "outer_scroller = driver.find_element(by = By.XPATH, value = '//*[@id=\"ng-app\"]')\n",
    "driver.execute_script(\"arguments[0].scrollTo(0,arguments[0].scrollHeight);\", outer_scroller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0471902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for path in industry_path.items():\n",
    "    # as i said above to scrape data by each industry, so here i will locate industry section from webpage.\n",
    "    # Now, Lets collect all the industries's XPATH to fetch data.\n",
    "    industry_dropdown = driver.find_element(by = By.XPATH, value = '/html/body/div[2]/div[2]/div/div[2]/div[1]/div/section[5]/div[2]/div/div[3]/div/div[1]/div[5]/span[2]')\n",
    "    time.sleep(5)\n",
    "    industry_dropdown.click()\n",
    "    industry = driver.find_element(by = By.XPATH, value = path[1])\n",
    "    industry.click() \n",
    "    \n",
    "    # Searching for inner scroller in a outer scroller.\n",
    "    inner_scroller = outer_scroller.find_element(by = By.XPATH, value = '//*[@id=\"row-4\"]/div[2]/div/div[4]/div[1]')\n",
    "    last_height = driver.execute_script(\"return arguments[0].scrollHeight;\", inner_scroller)\n",
    "    \n",
    "    # Running a Infinite Loop\n",
    "    for i in iter(int, 1):\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight);\", inner_scroller)\n",
    "        time.sleep(10)\n",
    "        new_height = driver.execute_script(\"return arguments[0].scrollHeight;\", inner_scroller)\n",
    "        \n",
    "        \n",
    "        # If last height and new height will be same, Break the infinite loop\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    \n",
    "    \n",
    "    image = []\n",
    "    rank = []\n",
    "    name = []\n",
    "    Net_Worth = []\n",
    "    age = []\n",
    "    source = []\n",
    "    country_territory = []\n",
    "    Industry = []\n",
    "    link = []\n",
    "\n",
    "    for i in soup.find_all('tr', 'ng-scope'):\n",
    "        try:\n",
    "            image.append(i.find('img')['src']) # image\n",
    "        except:\n",
    "            image.append('NaN')\n",
    "        try:    \n",
    "            rank.append(i.find('td','rank').find('span','ng-binding').text) #rank\n",
    "        except:\n",
    "            rank.append(\"NaN\")\n",
    "        try:    \n",
    "            name.append(i.find('td','name').h3.text) #name\n",
    "        except:\n",
    "            name.append(\"NaN\")\n",
    "        try:    \n",
    "            Net_Worth.append(i.find('td','Net').text) #net_worth   \n",
    "        except:\n",
    "            Net_Worth.append(\"NaN\")    \n",
    "        try:    \n",
    "            age.append(i.find('td','age').text) #age \n",
    "        except:\n",
    "            age.append(\"NaN\")\n",
    "\n",
    "        try:    \n",
    "            source.append(i.find('td','source').text) # source    \n",
    "        except:\n",
    "            source.append(\"NaN\")     \n",
    "\n",
    "        try:    \n",
    "            country_territory.append(i.find('td','Country/Territory').text) # Country\n",
    "        except:\n",
    "            country_territory.append(\"NaN\")  \n",
    "            \n",
    "        try:\n",
    "            link.append(i.find('h3').a.get('href')) # Name's internal link to scrape more information again.\n",
    "        except:\n",
    "            link.append(np.nan)\n",
    "    \n",
    "    # If path's name is \"Automotive\", Create a DataFrame\n",
    "    if path[0] == 'Automotive':\n",
    "        x = pd.DataFrame({\n",
    "            \"Image\":image,\n",
    "            \"Rank\":rank,\n",
    "            \"Name\":name,\n",
    "            \"Net_Worth\":Net_Worth,\n",
    "            \"Age\":age,\n",
    "            \"Source\":source,\n",
    "            \"Country\":country_territory,\n",
    "            \"internal_link\":link,\n",
    "            \"Industry\":path[0]\n",
    "       })\n",
    "        \n",
    "    # If path is other than automotive, concat the below dataframe to variable x    \n",
    "    else:\n",
    "        x = pd.concat([x,\n",
    "                  pd.DataFrame({\n",
    "                    \"Image\":image,\n",
    "                    \"Rank\":rank,\n",
    "                    \"Name\":name,\n",
    "                    \"Net_Worth\":Net_Worth,\n",
    "                    \"Age\":age,\n",
    "                    \"Source\":source,\n",
    "                    \"Country\":country_territory,\n",
    "                    \"internal_link\":link,\n",
    "                    \"Industry\":path[0]\n",
    "                 }) ], ignore_index=True)\n",
    "        \n",
    "print(\"Done\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675063df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(\"billionare_file.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae0739",
   "metadata": {},
   "source": [
    "## Now lets extract female's billionares list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa36f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service('C:/Users/Vivek Kumar/Desktop/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.forbes.com/real-time-billionaires/#42e2c8563d78')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1855d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating Female XPATH.\n",
    "female_page = driver.find_element(by = By.XPATH, value= '//*[@id=\"row-4\"]/div[2]/div/div[3]/div/div[1]/div[4]')\n",
    "female_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e2990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "# Locate for outer scroller first, if you want to locate inner scroller\n",
    "outer_scroller = driver.find_element(by = By.XPATH, value = '//*[@id=\"ng-app\"]')\n",
    "# scroll to end\n",
    "driver.execute_script(\"arguments[0].scrollTo(0,arguments[0].scrollHeight);\", outer_scroller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d01777",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "# Locating inner scroller inside outer scroler\n",
    "inner_scroller = outer_scroller.find_element(by = By.XPATH, value = '//*[@id=\"row-4\"]/div[2]/div/div[4]/div[1]')\n",
    "# calculate webpage height before scrolling\n",
    "last_height = driver.execute_script(\"return arguments[0].scrollHeight;\", inner_scroller)\n",
    "# running a infinite loop\n",
    "for i in iter(int, 1):\n",
    "    #scrolling inner scroller to infinite to fetch html page.\n",
    "    driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight);\", inner_scroller)\n",
    "    time.sleep(10)\n",
    "    new_height = driver.execute_script(\"return arguments[0].scrollHeight;\", inner_scroller)\n",
    "\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70a9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all html file\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b909649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.forbes.com/profile/francoise-bettencourt-meyers/?list=rtb/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h3').a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618d7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_name = []\n",
    "for i in soup.find_all('tr', 'ng-scope'):\n",
    "    try:    \n",
    "        female_name.append(i.find('td','name').h3.text)    \n",
    "    except:\n",
    "        female_name.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9486724",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({\n",
    "    'female_name':female_name\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c905b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('female_billionares.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09d252",
   "metadata": {},
   "source": [
    "##### So I scraped all the data from this link \"https://www.forbes.com/real-time-billionaires/#3b1023cd3d78\", which include males data and females data both. Further I will merge both the datasets and will clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfc5d6",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
